# Qwen3 1.7B å¤šèªè¨€ç¿»è­¯æ¨¡å‹å¾®èª¿ç³»çµ± - ä½¿ç”¨æ•™å­¸æ–‡æª”

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæ˜¯åŸºæ–¼ **Qwen3-1.7B** æ¨¡å‹çš„å¤šèªè¨€ç¿»è­¯ç³»çµ±ï¼Œä½¿ç”¨ **QLoRA** (é‡åŒ– LoRA) æŠ€è¡“é€²è¡Œé«˜æ•ˆå¾®èª¿ï¼Œæ”¯æ´**ç¹é«”ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡**ä¹‹é–“çš„é›™å‘ç¿»è­¯ã€‚

---

## ğŸ”§ æ ¸å¿ƒç¨‹å¼åŠŸèƒ½èªªæ˜

### 1. `Preprocess_Data_Moses.py` - è³‡æ–™é è™•ç†

**åŠŸèƒ½**ï¼šå°‡ Moses æ ¼å¼çš„å¹³è¡Œèªæ–™è½‰æ›ç‚ºæ¨¡å‹è¨“ç·´æ‰€éœ€çš„ tokenized æ ¼å¼

**ä¸»è¦åŠŸèƒ½**ï¼š

- æƒæ `_dataset/` ç›®éŒ„ï¼Œè‡ªå‹•ç™¼ç¾å¹³è¡Œèªæ–™å°ï¼ˆå¦‚ corpus.en + corpus.zh_twï¼‰
- è‡ªå‹•é…å°ä¸åŒèªè¨€çš„æª”æ¡ˆ
- ç”Ÿæˆé›™å‘ç¿»è­¯æ¨£æœ¬ï¼ˆAâ†’B å’Œ Bâ†’Aï¼‰
- æ·»åŠ èªè¨€æ¨™ç±¤ï¼ˆ`<en>` `<zh_TW>` `<ja>`ï¼‰
- ä½¿ç”¨ Qwen3 tokenizer é€²è¡Œ tokenization
- ä¿å­˜ç‚º Arrow æ ¼å¼åˆ° `qwen3_cje_lora/tokenized_*/`

**æ”¯æ´çš„èªè¨€æ“´å±•å**ï¼š`.en` (è‹±æ–‡)ã€`.ja` (æ—¥æ–‡)ã€`.zh` / `.zh_tw` / `.zh_hant` (ç¹é«”ä¸­æ–‡)

**ä½¿ç”¨æ–¹å¼**ï¼š

```bash
python Preprocess_Data_Moses.py
# æˆ–æŒ‡å®šè³‡æ–™ç›®éŒ„
python Preprocess_Data_Moses.py --data_root /path/to/data
```

---

### 2. `finetune_lora.py` - LoRA å¾®èª¿è¨“ç·´

**åŠŸèƒ½**ï¼šä½¿ç”¨é è™•ç†çš„è³‡æ–™è¨“ç·´ LoRA é©é…å™¨

**ä¸»è¦åŠŸèƒ½**ï¼š

- åŠ è¼‰ Qwen3-1.7B åŸºç¤æ¨¡å‹ï¼ˆ4-bit é‡åŒ–ï¼‰
- é…ç½® LoRA åƒæ•¸ï¼ˆr=32, alpha=64, dropout=0.05ï¼‰
- ç›£æ§ `qwen3_cje_lora/tokenized_*` ç›®éŒ„ï¼Œè³‡æ–™æº–å‚™å¥½å¾Œè‡ªå‹•å•Ÿå‹•è¨“ç·´
- ä½¿ç”¨ QLoRA æŠ€è¡“é€²è¡Œåƒæ•¸é«˜æ•ˆå¾®èª¿
- å®šæœŸä¿å­˜æª¢æŸ¥é»
- æ”¯æ´é©—è­‰é›†è©•ä¼°

**è¨“ç·´é…ç½®**ï¼š

- Batch size: 64ï¼ˆæœ‰æ•ˆï¼‰= 8ï¼ˆæ¯è£ç½®ï¼‰Ã— 8ï¼ˆæ¢¯åº¦ç´¯ç©ï¼‰
- Learning rate: 2e-4
- Optimizer: paged_adamw_8bit
- é è¨­è¨“ç·´ 3 å€‹ epoch

**ä½¿ç”¨æ–¹å¼**ï¼š

```bash
python finetune_lora.py
# ä½¿ç”¨éƒ¨åˆ†è³‡æ–™è¨“ç·´ï¼ˆå¦‚ 10%ï¼‰
python finetune_lora.py --subset 0.1 --epochs 5
```

---

### 3. `merge_Qwen3_with_lora.py` - æ¨¡å‹åˆä½µ

**åŠŸèƒ½**ï¼šå°‡è¨“ç·´å¥½çš„ LoRA é©é…å™¨åˆä½µå›åŸºç¤æ¨¡å‹

**ä¸»è¦åŠŸèƒ½**ï¼š

- åŠ è¼‰ Qwen3-1.7B åŸºç¤æ¨¡å‹
- åŠ è¼‰è¨“ç·´å¥½çš„ LoRA é©é…å™¨
- åŸ·è¡Œæ¬Šé‡åˆä½µï¼ˆ`merge_and_unload()`ï¼‰
- ä¿å­˜å®Œæ•´çš„å¾®èª¿æ¨¡å‹å’Œ tokenizer
- è¼¸å‡ºå¯ç›´æ¥ç”¨æ–¼æ¨ç†çš„å®Œæ•´æ¨¡å‹

**ä½¿ç”¨æ–¹å¼**ï¼š

```bash
python merge_Qwen3_with_lora.py
```

**æ³¨æ„**ï¼šéœ€åœ¨è…³æœ¬ä¸­é…ç½®æ­£ç¢ºçš„è·¯å¾‘ï¼š

- `base_model_path`ï¼šåŸºç¤æ¨¡å‹è·¯å¾‘
- `adapter_path`ï¼šLoRA é©é…å™¨è·¯å¾‘
- `output_merged_model_path`ï¼šè¼¸å‡ºè·¯å¾‘

---

### 4. `model_evaluate.py` - æ¨¡å‹è©•ä¼°

**åŠŸèƒ½**ï¼šä½¿ç”¨ BLEU æŒ‡æ¨™è©•ä¼°ç¿»è­¯è³ªé‡

**ä¸»è¦åŠŸèƒ½**ï¼š

- åŠ è¼‰åˆä½µå¾Œçš„æ¨¡å‹
- æº–å‚™æ¸¬è©¦é›†ï¼ˆæºèªè¨€å¥å­ + åƒè€ƒç¿»è­¯ï¼‰
- ç”Ÿæˆæ¨¡å‹ç¿»è­¯
- è¨ˆç®— BLEU åˆ†æ•¸å’Œç›¸é—œæŒ‡æ¨™
- é¡¯ç¤ºè©³ç´°çš„è©•ä¼°çµæœ

**è©•ä¼°æŒ‡æ¨™**ï¼š

- BLEU åˆ†æ•¸
- å„ n-gram æº–ç¢ºç‡
- ç°¡æ½”æ‡²ç½°ï¼ˆBrevity Penaltyï¼‰
- ç¿»è­¯é•·åº¦çµ±è¨ˆ

**ä½¿ç”¨æ–¹å¼**ï¼š

```bash
python model_evaluate.py
```

### 6. `main.py` - ä¸»ç¨‹å¼

**åŠŸèƒ½**ï¼šæ•´åˆæ¨ç†åŠŸèƒ½çš„ä¸»ç¨‹å¼

**ä¸»è¦åŠŸèƒ½**ï¼š

- è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
- æä¾›ç¿»è­¯ä»‹é¢
- æ”¯æ´äº’å‹•å¼ç¿»è­¯

---

## ğŸš€ åŸºæœ¬ä½¿ç”¨æµç¨‹

```
æ­¥é©Ÿ 1: æº–å‚™è³‡æ–™ â†’ å°‡å¹³è¡Œèªæ–™æ”¾å…¥ _dataset/ ç›®éŒ„

æ­¥é©Ÿ 2: è³‡æ–™é è™•ç† â†’ python Preprocess_Data_Moses.py

æ­¥é©Ÿ 3: LoRA å¾®èª¿ â†’ python finetune_lora.py

æ­¥é©Ÿ 4: åˆä½µæ¨¡å‹ â†’ python merge_Qwen3_with_lora.py

æ­¥é©Ÿ 5: è©•ä¼°æ¨¡å‹ â†’ python model_evaluate.py
```

---

## ğŸ’¡ æ¨¡å‹ä½¿ç”¨ç¯„ä¾‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# è¼‰å…¥æ¨¡å‹
model_path = "./merged_qwen3_1_7b_with_lora"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    trust_remote_code=True
)

# ç¿»è­¯å‡½æ•¸
def translate(source_lang, target_lang, text):
    prompt = f"""<|im_start|>system
You are a precise and helpful multilingual translation assistant.<|im_end|>
<|im_start|>user
<{source_lang}> <{target_lang}>
{text}<|im_end|>
<|im_start|>assistant
"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.split("<|im_start|>assistant")[-1].strip()

# ä½¿ç”¨ç¯„ä¾‹
print(translate("zh_tw", "en", "ä»Šå¤©å¤©æ°£çœŸå¥½ã€‚"))
print(translate("en", "ja", "Hello, how are you?"))
```

---

## ğŸ“š åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡æª”

- **Qwen å®˜æ–¹æ–‡æª”**: https://github.com/QwenLM/Qwen
- **Hugging Face Transformers**: https://huggingface.co/docs/transformers
- **PEFT å‡½å¼åº«**: https://github.com/huggingface/peft

### ç›¸é—œè«–æ–‡

- **LoRA è«–æ–‡**: https://arxiv.org/abs/2106.09685
- **QLoRA è«–æ–‡**: https://arxiv.org/abs/2305.14314

### å·¥å…·èˆ‡å‡½å¼åº«

- **llama.cpp**: https://github.com/ggerganov/llama.cpp ï¼ˆç”¨æ–¼ GGUF æ ¼å¼è½‰æ›ï¼‰
- **Datasets å‡½å¼åº«**: https://huggingface.co/docs/datasets
- **TRL å‡½å¼åº«**: https://github.com/huggingface/trl

---
